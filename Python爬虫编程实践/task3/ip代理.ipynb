{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么会出现IP被封\n",
    "\n",
    "网站为了防止被爬取，会有反爬机制，对于同一个IP地址的大量同类型的访问，会封锁IP，过一段时间后，才能继续访问\n",
    "\n",
    "## 如何应对IP被封的问题\n",
    "\n",
    "有几种套路：\n",
    "\n",
    "1. 修改请求头，模拟浏览器（而不是代码去直接访问）去访问\n",
    "2. 采用代理IP并轮换\n",
    "3. 设置访问时间间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何获取代理IP地址\n",
    "\n",
    "- 从该网站获取： https://www.xicidaili.com/\n",
    "- inspect -> 鼠标定位：\n",
    "- 要获取的代理IP地址，属于class = \"odd\"标签的内容：代码如下，获取的代理IP保存在proxy_ip_list列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HTTP://222.95.144.128:3000', 'HTTP://222.85.28.130:40505', 'HTTP://121.237.148.248:3000', 'HTTPS://121.237.149.109:3000', 'HTTP://222.95.144.195:3000', 'HTTP://121.237.149.88:3000', 'HTTP://121.237.148.192:3000', 'HTTPS://121.237.148.183:3000', 'HTTP://121.237.149.56:3000', 'HTTP://121.237.149.163:3000', 'HTTP://122.228.19.8:3389', 'HTTP://116.196.87.86:20183', 'HTTP://112.95.205.137:8888', 'HTTP://61.153.251.150:22222', 'HTTPS://113.140.1.82:53281', 'HTTP://218.27.136.169:8085', 'HTTP://211.147.226.4:8118', 'HTTP://118.114.166.99:8118', 'HTTPS://223.245.35.187:65309', 'HTTPS://163.125.67.66:9797', 'HTTPS://222.95.144.129:3000', 'HTTPS://222.95.144.119:3000', 'HTTPS://222.95.144.200:3000', 'HTTPS://121.237.149.26:3000', 'HTTPS://117.88.176.158:3000', 'HTTPS://122.228.19.9:3389', 'HTTPS://117.88.177.124:3000', 'HTTPS://119.41.206.207:53281', 'HTTPS://121.237.148.59:3000', 'HTTPS://119.254.94.93:46323', 'HTTP://222.95.144.128:3000', 'HTTP://122.228.19.8:3389', 'HTTP://59.38.62.148:9797', 'HTTP://121.237.148.115:3000', 'HTTP://121.237.149.16:3000', 'HTTP://112.247.169.33:8060', 'HTTP://116.196.87.86:20183', 'HTTP://117.88.177.34:3000', 'HTTP://121.237.149.160:3000', 'HTTP://121.237.149.73:3000']\n"
     ]
    }
   ],
   "source": [
    "# 案例代码\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def open_proxy_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers, timeout = 20)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return(r.text)\n",
    "    except:\n",
    "        print('无法访问网页' + url)\n",
    "\n",
    "\n",
    "def get_proxy_ip(response):\n",
    "    proxy_ip_list = []\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    proxy_ips  = soup.select('.odd')#选择标签\n",
    "    for proxy_ip in proxy_ips:\n",
    "        ip = proxy_ip.select('td')[1].text\n",
    "        port = proxy_ip.select('td')[2].text\n",
    "        protocol = proxy_ip.select('td')[5].text\n",
    "        if protocol in ('HTTP','HTTPS'):\n",
    "            proxy_ip_list.append(f'{protocol}://{ip}:{port}')\n",
    "    return proxy_ip_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    proxy_url = 'https://www.xicidaili.com/'\n",
    "    text = open_proxy_url(proxy_url)\n",
    "    proxy_ip_filename = 'proxy_ip.txt'\n",
    "    with open(proxy_ip_filename, 'w') as f:\n",
    "        f.write(text)\n",
    "    text = open(proxy_ip_filename, 'r').read()\n",
    "    proxy_ip_list = get_proxy_ip(text)\n",
    "    print(proxy_ip_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取如下数据：\n",
    "\n",
    "获取到代理IP地址后，发现数据缺失很多，再仔细查看elements，发现有些并非class = “odd”，而是…，这些数据没有被获取\n",
    "class = \"odd\"奇数的结果，而没有class = \"odd\"的是偶数的结果\n",
    "\n",
    "通过bs4的find_all(‘tr’)来获取所有IP："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy_ip(response):\n",
    "    proxy_ip_list = []\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    proxy_ips = soup.find(id = 'ip_list').find_all('tr')\n",
    "    for proxy_ip in proxy_ips:\n",
    "        if len(proxy_ip.select('td')) >=8:\n",
    "            ip = proxy_ip.select('td')[1].text\n",
    "            port = proxy_ip.select('td')[2].text\n",
    "            protocol = proxy_ip.select('td')[5].text\n",
    "            if protocol in ('HTTP','HTTPS','http','https'):\n",
    "                proxy_ip_list.append(f'{protocol}://{ip}:{port}')\n",
    "    return proxy_ip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用代理\n",
    "- proxies的格式是一个字典：\n",
    "- {‘http’: ‘http://IP:port‘,‘https’:'https://IP:port‘}\n",
    "- 把它直接传入requests的get方法中即可\n",
    "- web_data = requests.get(url, headers=headers, proxies=proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url_using_proxy(url, proxy):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    proxies = {}\n",
    "    if proxy.startswith('HTTPS'):\n",
    "        proxies['https'] = proxy\n",
    "    else:\n",
    "        proxies['http'] = proxy\n",
    "try:\n",
    "    r = requests.get(url, headers = headers, proxies = proxies, timeout = 10)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    return (r.text, r.status_code)\n",
    "except:\n",
    "    print('无法访问网页' + url)\n",
    "    return False\n",
    "url = 'http://www.baidu.com'\n",
    "text = open_url_using_proxy(url, proxy_ip_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确认代理IP地址有效性\n",
    "- 无论是免费还是收费的代理网站，提供的代理IP都未必有效，我们应该验证一下，有效后，再放入我们的代理IP池中，以下通过几种方式：访问网站，得到的返回码是200真正的访问某些网站，获取title等，验证title与预计的相同访问某些可以提供被访问IP的网站，类似于“查询我的IP”的网站，查看返回的IP地址是什么验证返回码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proxy_avaliability(proxy):\n",
    "    url = 'http://www.baidu.com'\n",
    "    result = open_url_using_proxy(url, proxy)\n",
    "    VALID_PROXY = False\n",
    "    if result:\n",
    "        text, status_code = result\n",
    "        if status_code == 200:\n",
    "            print('有效代理IP: ' + proxy)\n",
    "        else:\n",
    "            print('无效代理IP: ' + proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进：确认网站title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show you code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proxy_avaliability(proxy):\n",
    "    url = 'http://www.baidu.com'\n",
    "    text, status_code = open_url_using_proxy(url, proxy)\n",
    "    VALID = False\n",
    "    if status_code == 200:\n",
    "        if r_title:\n",
    "            if r_title[0] == '<title>百度一下，你就知道</title>':\n",
    "                VALID = True\n",
    "    if VALID:\n",
    "        print('有效代理IP: ' + proxy)\n",
    "    else:\n",
    "        print('无效代理IP: ' + proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def open_proxy_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers, timeout = 10)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        print('无法访问网页' + url)\n",
    "\n",
    "\n",
    "def get_proxy_ip(response):\n",
    "    proxy_ip_list = []\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    proxy_ips = soup.find(id = 'ip_list').find_all('tr')\n",
    "    for proxy_ip in proxy_ips:\n",
    "        if len(proxy_ip.select('td')) >=8:\n",
    "            ip = proxy_ip.select('td')[1].text\n",
    "            port = proxy_ip.select('td')[2].text\n",
    "            protocol = proxy_ip.select('td')[5].text\n",
    "            if protocol in ('HTTP','HTTPS','http','https'):\n",
    "                proxy_ip_list.append(f'{protocol}://{ip}:{port}')\n",
    "    return proxy_ip_list\n",
    "\n",
    "\n",
    "def open_url_using_proxy(url, proxy):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    proxies = {}\n",
    "    if proxy.startswith(('HTTPS','https')):\n",
    "        proxies['https'] = proxy\n",
    "    else:\n",
    "        proxies['http'] = proxy\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers, proxies = proxies, timeout = 10)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return (r.text, r.status_code)\n",
    "    except:\n",
    "        print('无法访问网页' + url)\n",
    "        print('无效代理IP: ' + proxy)\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_proxy_avaliability(proxy):\n",
    "    url = 'http://www.baidu.com'\n",
    "    result = open_url_using_proxy(url, proxy)\n",
    "    VALID_PROXY = False\n",
    "    if result:\n",
    "        text, status_code = result\n",
    "        if status_code == 200:\n",
    "            r_title = re.findall('<title>.*</title>', text)\n",
    "            if r_title:\n",
    "                if r_title[0] == '<title>百度一下，你就知道</title>':\n",
    "                    VALID_PROXY = True\n",
    "        if VALID_PROXY:\n",
    "            check_ip_url = 'https://jsonip.com/'\n",
    "            try:\n",
    "                text, status_code = open_url_using_proxy(check_ip_url, proxy)\n",
    "            except:\n",
    "                return\n",
    "\n",
    "            print('有效代理IP: ' + proxy)\n",
    "            with open('valid_proxy_ip.txt','a') as f:\n",
    "                f.writelines(proxy)\n",
    "            try:\n",
    "                source_ip = json.loads(text).get('ip')\n",
    "                print(f'源IP地址为：{source_ip}')\n",
    "                print('='*40)\n",
    "            except:\n",
    "                print('返回的非json,无法解析')\n",
    "                print(text)\n",
    "    else:\n",
    "        print('无效代理IP: ' + proxy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    proxy_url = 'https://www.xicidaili.com/'\n",
    "    proxy_ip_filename = 'proxy_ip.txt'\n",
    "    text = open(proxy_ip_filename, 'r').read()\n",
    "    proxy_ip_list = get_proxy_ip(text)\n",
    "    for proxy in proxy_ip_list:\n",
    "        check_proxy_avaliability(proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关于http和https代理\n",
    "- 可以看到proxies中有两个键值对：\n",
    "- {‘http’: ‘http://IP:port‘,‘https’:'https://IP:port‘}\n",
    "- 其中 HTTP 代理，只代理 HTTP 网站，对于 HTTPS 的网站不起作用，也就是说，用的是本机 IP，反之亦然。\n",
    "- 我刚才使用的验证的网站是https://jsonip.com, 是HTTPS网站所以探测到的有效代理中，如果是https代理，则返回的是代理地址\n",
    "- 如果是http代理，将使用本机IP进行访问，返回的是我的公网IP地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
