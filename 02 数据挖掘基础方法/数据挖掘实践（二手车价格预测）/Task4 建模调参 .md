
# Datawhale é›¶åŸºç¡€å…¥é—¨æ•°æ®æŒ–æ˜-Task4 å»ºæ¨¡è°ƒå‚ 

## å››ã€å»ºæ¨¡ä¸è°ƒå‚

Tip:æ­¤éƒ¨åˆ†ä¸ºé›¶åŸºç¡€å…¥é—¨æ•°æ®æŒ–æ˜çš„ Task4 å»ºæ¨¡è°ƒå‚ éƒ¨åˆ†ï¼Œå¸¦ä½ æ¥äº†è§£å„ç§æ¨¡å‹ä»¥åŠæ¨¡å‹çš„è¯„ä»·å’Œè°ƒå‚ç­–ç•¥ï¼Œæ¬¢è¿å¤§å®¶åç»­å¤šå¤šäº¤æµã€‚

**èµ›é¢˜ï¼šé›¶åŸºç¡€å…¥é—¨æ•°æ®æŒ–æ˜ - äºŒæ‰‹è½¦äº¤æ˜“ä»·æ ¼é¢„æµ‹**

åœ°å€ï¼šhttps://tianchi.aliyun.com/competition/entrance/231784/introduction?spm=5176.12281957.1004.1.38b02448ausjSX 
    

## 5.1 å­¦ä¹ ç›®æ ‡

* äº†è§£å¸¸ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶æŒæ¡æœºå™¨å­¦ä¹ æ¨¡å‹çš„å»ºæ¨¡ä¸è°ƒå‚æµç¨‹
* å®Œæˆç›¸åº”å­¦ä¹ æ‰“å¡ä»»åŠ¡

## 5.2 å†…å®¹ä»‹ç»

1. çº¿æ€§å›å½’æ¨¡å‹ï¼š
    - çº¿æ€§å›å½’å¯¹äºç‰¹å¾çš„è¦æ±‚ï¼›
    - å¤„ç†é•¿å°¾åˆ†å¸ƒï¼›
    - ç†è§£çº¿æ€§å›å½’æ¨¡å‹ï¼›
2. æ¨¡å‹æ€§èƒ½éªŒè¯ï¼š
    - è¯„ä»·å‡½æ•°ä¸ç›®æ ‡å‡½æ•°ï¼›
    - äº¤å‰éªŒè¯æ–¹æ³•ï¼›
    - ç•™ä¸€éªŒè¯æ–¹æ³•ï¼›
    - é’ˆå¯¹æ—¶é—´åºåˆ—é—®é¢˜çš„éªŒè¯ï¼›
    - ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ï¼›
    - ç»˜åˆ¶éªŒè¯æ›²çº¿ï¼›
3. åµŒå…¥å¼ç‰¹å¾é€‰æ‹©ï¼š
    - Lassoå›å½’ï¼›
    - Ridgeå›å½’ï¼›
    - å†³ç­–æ ‘ï¼›
4. æ¨¡å‹å¯¹æ¯”ï¼š
    - å¸¸ç”¨çº¿æ€§æ¨¡å‹ï¼›
    - å¸¸ç”¨éçº¿æ€§æ¨¡å‹ï¼›
5. æ¨¡å‹è°ƒå‚ï¼š
    - è´ªå¿ƒè°ƒå‚æ–¹æ³•ï¼›
    - ç½‘æ ¼è°ƒå‚æ–¹æ³•ï¼›
    - è´å¶æ–¯è°ƒå‚æ–¹æ³•ï¼›

## 5.3 ç›¸å…³åŸç†ä»‹ç»ä¸æ¨è

ç”±äºç›¸å…³ç®—æ³•åŸç†ç¯‡å¹…è¾ƒé•¿ï¼Œæœ¬æ–‡æ¨èäº†ä¸€äº›åšå®¢ä¸æ•™æä¾›åˆå­¦è€…ä»¬è¿›è¡Œå­¦ä¹ ã€‚

### 5.3.1 çº¿æ€§å›å½’æ¨¡å‹

https://zhuanlan.zhihu.com/p/49480391

### 5.3.2 å†³ç­–æ ‘æ¨¡å‹

https://zhuanlan.zhihu.com/p/65304798

### 5.3.3 GBDTæ¨¡å‹

https://zhuanlan.zhihu.com/p/45145899

### 5.3.4 XGBoostæ¨¡å‹

https://zhuanlan.zhihu.com/p/86816771

### 5.3.5 LightGBMæ¨¡å‹

https://zhuanlan.zhihu.com/p/89360721

### 5.3.6 æ¨èæ•™æï¼š

   - ã€Šæœºå™¨å­¦ä¹ ã€‹ https://book.douban.com/subject/26708119/
   - ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ https://book.douban.com/subject/10590856/
   - ã€ŠPythonå¤§æˆ˜æœºå™¨å­¦ä¹ ã€‹ https://book.douban.com/subject/26987890/
   - ã€Šé¢å‘æœºå™¨å­¦ä¹ çš„ç‰¹å¾å·¥ç¨‹ã€‹ https://book.douban.com/subject/26826639/
   - ã€Šæ•°æ®ç§‘å­¦å®¶è®¿è°ˆå½•ã€‹ https://book.douban.com/subject/30129410/


## 5.4 ä»£ç ç¤ºä¾‹

### 5.4.1 è¯»å–æ•°æ®


```python
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
```

reduce_mem_usage å‡½æ•°é€šè¿‡è°ƒæ•´æ•°æ®ç±»å‹ï¼Œå¸®åŠ©æˆ‘ä»¬å‡å°‘æ•°æ®åœ¨å†…å­˜ä¸­å ç”¨çš„ç©ºé—´


```python
def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() 
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() 
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return df
```


```python
sample_feature = reduce_mem_usage(pd.read_csv('data_for_tree.csv'))
```

    Memory usage of dataframe is 60507328.00 MB
    Memory usage after optimization is: 15724107.00 MB
    Decreased by 74.0%
    


```python
continuous_feature_names = [x for x in sample_feature.columns if x not in ['price','brand','model','brand']]
```

### 5.4.2 çº¿æ€§å›å½’ & äº”æŠ˜äº¤å‰éªŒè¯ & æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡æƒ…å†µ


```python
sample_feature = sample_feature.dropna().replace('-', 0).reset_index(drop=True)
sample_feature['notRepairedDamage'] = sample_feature['notRepairedDamage'].astype(np.float32)
train = sample_feature[continuous_feature_names + ['price']]

train_X = train[continuous_feature_names]
train_y = train['price']
```

#### 5.4.2 - 1 ç®€å•å»ºæ¨¡


```python
from sklearn.linear_model import LinearRegression
```


```python
model = LinearRegression(normalize=True)
```


```python
model = model.fit(train_X, train_y)
```

æŸ¥çœ‹è®­ç»ƒçš„çº¿æ€§å›å½’æ¨¡å‹çš„æˆªè·ï¼ˆinterceptï¼‰ä¸æƒé‡(coef)


```python
'intercept:'+ str(model.intercept_)

sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```




    [('v_6', 3342612.384537345),
     ('v_8', 684205.534533214),
     ('v_9', 178967.94192530424),
     ('v_7', 35223.07319016895),
     ('v_5', 21917.550249749802),
     ('v_3', 12782.03250792227),
     ('v_12', 11654.925634146672),
     ('v_13', 9884.194615297649),
     ('v_11', 5519.182176035517),
     ('v_10', 3765.6101415594258),
     ('gearbox', 900.3205339198406),
     ('fuelType', 353.5206495542567),
     ('bodyType', 186.51797317460046),
     ('city', 45.17354204168846),
     ('power', 31.163045441455335),
     ('brand_price_median', 0.535967111869784),
     ('brand_price_std', 0.4346788365040235),
     ('brand_amount', 0.15308295553300566),
     ('brand_price_max', 0.003891831020467389),
     ('seller', -1.2684613466262817e-06),
     ('offerType', -4.759058356285095e-06),
     ('brand_price_sum', -2.2430642281682917e-05),
     ('name', -0.00042591632723759166),
     ('used_time', -0.012574429533889028),
     ('brand_price_average', -0.414105722833381),
     ('brand_price_min', -2.3163823428971835),
     ('train', -5.392535065078232),
     ('power_bin', -59.24591853031839),
     ('v_14', -233.1604256172217),
     ('kilometer', -372.96600915402496),
     ('notRepairedDamage', -449.29703564695365),
     ('v_0', -1490.6790578168238),
     ('v_4', -14219.648899108111),
     ('v_2', -16528.55239086934),
     ('v_1', -42869.43976200439)]




```python
from matplotlib import pyplot as plt
```


```python
subsample_index = np.random.randint(low=0, high=len(train_y), size=50)
```

ç»˜åˆ¶ç‰¹å¾v_9çš„å€¼ä¸æ ‡ç­¾çš„æ•£ç‚¹å›¾ï¼Œå›¾ç‰‡å‘ç°æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆè“è‰²ç‚¹ï¼‰ä¸çœŸå®æ ‡ç­¾ï¼ˆé»‘è‰²ç‚¹ï¼‰çš„åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œä¸”éƒ¨åˆ†é¢„æµ‹å€¼å‡ºç°äº†å°äº0çš„æƒ…å†µï¼Œè¯´æ˜æˆ‘ä»¬çš„æ¨¡å‹å­˜åœ¨ä¸€äº›é—®é¢˜


```python
plt.scatter(train_X['v_9'][subsample_index], train_y[subsample_index], color='black')
plt.scatter(train_X['v_9'][subsample_index], model.predict(train_X.loc[subsample_index]), color='blue')
plt.xlabel('v_9')
plt.ylabel('price')
plt.legend(['True Price','Predicted Price'],loc='upper right')
print('The predicted price is obvious different from true price')
plt.show()
```

    The predicted price is obvious different from true price
    


![output_22_1](https://img-blog.csdnimg.cn/20200321231804889.png)



é€šè¿‡ä½œå›¾æˆ‘ä»¬å‘ç°æ•°æ®çš„æ ‡ç­¾ï¼ˆpriceï¼‰å‘ˆç°é•¿å°¾åˆ†å¸ƒï¼Œä¸åˆ©äºæˆ‘ä»¬çš„å»ºæ¨¡é¢„æµ‹ã€‚åŸå› æ˜¯å¾ˆå¤šæ¨¡å‹éƒ½å‡è®¾æ•°æ®è¯¯å·®é¡¹ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œè€Œé•¿å°¾åˆ†å¸ƒçš„æ•°æ®è¿èƒŒäº†è¿™ä¸€å‡è®¾ã€‚å‚è€ƒåšå®¢ï¼šhttps://blog.csdn.net/Noob_daniel/article/details/76087829


```python
import seaborn as sns
print('It is clear to see the price shows a typical exponential distribution')
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y)
plt.subplot(1,2,2)
sns.distplot(train_y[train_y < np.quantile(train_y, 0.9)])
```

    It is clear to see the price shows a typical exponential distribution
    




    <matplotlib.axes._subplots.AxesSubplot at 0x1b33efb2f98>




![output_24_2](https://img-blog.csdnimg.cn/20200321231820197.png)


åœ¨è¿™é‡Œæˆ‘ä»¬å¯¹æ ‡ç­¾è¿›è¡Œäº† $log(x+1)$ å˜æ¢ï¼Œä½¿æ ‡ç­¾è´´è¿‘äºæ­£æ€åˆ†å¸ƒ


```python
train_y_ln = np.log(train_y + 1)
```


```python
import seaborn as sns
print('The transformed price seems like normal distribution')
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y_ln)
plt.subplot(1,2,2)
sns.distplot(train_y_ln[train_y_ln < np.quantile(train_y_ln, 0.9)])
```

    The transformed price seems like normal distribution
    




    <matplotlib.axes._subplots.AxesSubplot at 0x1b33f077160>




![output_27_2](https://img-blog.csdnimg.cn/20200321231840673.png)



```python
model = model.fit(train_X, train_y_ln)

print('intercept:'+ str(model.intercept_))
sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```

    intercept:23.515920686637713
    




    [('v_9', 6.043993029165403),
     ('v_12', 2.0357439855551394),
     ('v_11', 1.3607608712255672),
     ('v_1', 1.3079816298861897),
     ('v_13', 1.0788833838535354),
     ('v_3', 0.9895814429387444),
     ('gearbox', 0.009170812023421397),
     ('fuelType', 0.006447089787635784),
     ('bodyType', 0.004815242907679581),
     ('power_bin', 0.003151801949447194),
     ('power', 0.0012550361843629999),
     ('train', 0.0001429273782925814),
     ('brand_price_min', 2.0721302299502698e-05),
     ('brand_price_average', 5.308179717783439e-06),
     ('brand_amount', 2.8308531339942507e-06),
     ('brand_price_max', 6.764442596115763e-07),
     ('offerType', 1.6765966392995324e-10),
     ('seller', 9.308109838457312e-12),
     ('brand_price_sum', -1.3473184925468486e-10),
     ('name', -7.11403461065247e-08),
     ('brand_price_median', -1.7608143661053008e-06),
     ('brand_price_std', -2.7899058266986454e-06),
     ('used_time', -5.6142735899344175e-06),
     ('city', -0.0024992974087053223),
     ('v_14', -0.012754139659375262),
     ('kilometer', -0.013999175312751872),
     ('v_0', -0.04553774829634237),
     ('notRepairedDamage', -0.273686961116076),
     ('v_7', -0.7455902679730504),
     ('v_4', -0.9281349233755761),
     ('v_2', -1.2781892166433606),
     ('v_5', -1.5458846136756323),
     ('v_10', -1.8059217242413748),
     ('v_8', -42.611729973490604),
     ('v_6', -241.30992120503035)]



å†æ¬¡è¿›è¡Œå¯è§†åŒ–ï¼Œå‘ç°é¢„æµ‹ç»“æœä¸çœŸå®å€¼è¾ƒä¸ºæ¥è¿‘ï¼Œä¸”æœªå‡ºç°å¼‚å¸¸çŠ¶å†µ


```python
plt.scatter(train_X['v_9'][subsample_index], train_y[subsample_index], color='black')
plt.scatter(train_X['v_9'][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color='blue')
plt.xlabel('v_9')
plt.ylabel('price')
plt.legend(['True Price','Predicted Price'],loc='upper right')
print('The predicted price seems normal after np.log transforming')
plt.show()
```

    The predicted price seems normal after np.log transforming
    


![output_30_1](https://img-blog.csdnimg.cn/20200321231902283.png)


#### 5.4.2 - 2 äº”æŠ˜äº¤å‰éªŒè¯

> åœ¨ä½¿ç”¨è®­ç»ƒé›†å¯¹å‚æ•°è¿›è¡Œè®­ç»ƒçš„æ—¶å€™ï¼Œç»å¸¸ä¼šå‘ç°äººä»¬é€šå¸¸ä¼šå°†ä¸€æ•´ä¸ªè®­ç»ƒé›†åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼ˆæ¯”å¦‚mnistæ‰‹å†™è®­ç»ƒé›†ï¼‰ã€‚ä¸€èˆ¬åˆ†ä¸ºï¼šè®­ç»ƒé›†ï¼ˆtrain_setï¼‰ï¼Œè¯„ä¼°é›†ï¼ˆvalid_setï¼‰ï¼Œæµ‹è¯•é›†ï¼ˆtest_setï¼‰è¿™ä¸‰ä¸ªéƒ¨åˆ†ã€‚è¿™å…¶å®æ˜¯ä¸ºäº†ä¿è¯è®­ç»ƒæ•ˆæœè€Œç‰¹æ„è®¾ç½®çš„ã€‚å…¶ä¸­æµ‹è¯•é›†å¾ˆå¥½ç†è§£ï¼Œå…¶å®å°±æ˜¯å®Œå…¨ä¸å‚ä¸è®­ç»ƒçš„æ•°æ®ï¼Œä»…ä»…ç”¨æ¥è§‚æµ‹æµ‹è¯•æ•ˆæœçš„æ•°æ®ã€‚è€Œè®­ç»ƒé›†å’Œè¯„ä¼°é›†åˆ™ç‰µæ¶‰åˆ°ä¸‹é¢çš„çŸ¥è¯†äº†ã€‚

>å› ä¸ºåœ¨å®é™…çš„è®­ç»ƒä¸­ï¼Œè®­ç»ƒçš„ç»“æœå¯¹äºè®­ç»ƒé›†çš„æ‹Ÿåˆç¨‹åº¦é€šå¸¸è¿˜æ˜¯æŒºå¥½çš„ï¼ˆåˆå§‹æ¡ä»¶æ•æ„Ÿï¼‰ï¼Œä½†æ˜¯å¯¹äºè®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦é€šå¸¸å°±ä¸é‚£ä¹ˆä»¤äººæ»¡æ„äº†ã€‚å› æ­¤æˆ‘ä»¬é€šå¸¸å¹¶ä¸ä¼šæŠŠæ‰€æœ‰çš„æ•°æ®é›†éƒ½æ‹¿æ¥è®­ç»ƒï¼Œè€Œæ˜¯åˆ†å‡ºä¸€éƒ¨åˆ†æ¥ï¼ˆè¿™ä¸€éƒ¨åˆ†ä¸å‚åŠ è®­ç»ƒï¼‰å¯¹è®­ç»ƒé›†ç”Ÿæˆçš„å‚æ•°è¿›è¡Œæµ‹è¯•ï¼Œç›¸å¯¹å®¢è§‚çš„åˆ¤æ–­è¿™äº›å‚æ•°å¯¹è®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®çš„ç¬¦åˆç¨‹åº¦ã€‚è¿™ç§æ€æƒ³å°±ç§°ä¸ºäº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰


```python
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error,  make_scorer
```


```python
def log_transfer(func):
    def wrapper(y, yhat):
        result = func(np.log(y), np.nan_to_num(np.log(yhat)))
        return result
    return wrapper
```


```python
scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, scoring=make_scorer(log_transfer(mean_absolute_error)))
```

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished
    

ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¯¹æœªå¤„ç†æ ‡ç­¾çš„ç‰¹å¾æ•°æ®è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆError 1.36ï¼‰


```python
print('AVG:', np.mean(scores))
```

    AVG: 1.3641908155886227
    

ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¯¹å¤„ç†è¿‡æ ‡ç­¾çš„ç‰¹å¾æ•°æ®è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆError 0.19ï¼‰


```python
scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error))
```

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished
    


```python
print('AVG:', np.mean(scores))
```

    AVG: 0.19382863663604424
    


```python
scores = pd.DataFrame(scores.reshape(1,-1))
scores.columns = ['cv' + str(x) for x in range(1, 6)]
scores.index = ['MAE']
scores
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cv1</th>
      <th>cv2</th>
      <th>cv3</th>
      <th>cv4</th>
      <th>cv5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MAE</th>
      <td>0.191642</td>
      <td>0.194986</td>
      <td>0.192737</td>
      <td>0.195329</td>
      <td>0.19445</td>
    </tr>
  </tbody>
</table>
</div>



#### 5.4.2 - 3 æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡æƒ…å†µ

ä½†åœ¨äº‹å®ä¸Šï¼Œç”±äºæˆ‘ä»¬å¹¶ä¸å…·æœ‰é¢„çŸ¥æœªæ¥çš„èƒ½åŠ›ï¼Œäº”æŠ˜äº¤å‰éªŒè¯åœ¨æŸäº›ä¸æ—¶é—´ç›¸å…³çš„æ•°æ®é›†ä¸Šåè€Œåæ˜ äº†ä¸çœŸå®çš„æƒ…å†µã€‚é€šè¿‡2018å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹2017å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å¯ä»¥é‡‡ç”¨æ—¶é—´é¡ºåºå¯¹æ•°æ®é›†è¿›è¡Œåˆ†éš”ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰ç”¨é å‰æ—¶é—´çš„4/5æ ·æœ¬å½“ä½œè®­ç»ƒé›†ï¼Œé åæ—¶é—´çš„1/5å½“ä½œéªŒè¯é›†ï¼Œæœ€ç»ˆç»“æœä¸äº”æŠ˜äº¤å‰éªŒè¯å·®è·ä¸å¤§


```python
import datetime
```


```python
sample_feature = sample_feature.reset_index(drop=True)
```


```python
split_point = len(sample_feature) // 5 * 4
```


```python
train = sample_feature.loc[:split_point].dropna()
val = sample_feature.loc[split_point:].dropna()

train_X = train[continuous_feature_names]
train_y_ln = np.log(train['price'] + 1)
val_X = val[continuous_feature_names]
val_y_ln = np.log(val['price'] + 1)
```


```python
model = model.fit(train_X, train_y_ln)
```


```python
mean_absolute_error(val_y_ln, model.predict(val_X))
```




    0.19443858353490887



#### 5.4.2 - 4 ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ä¸éªŒè¯æ›²çº¿


```python
from sklearn.model_selection import learning_curve, validation_curve
```


```python
? learning_curve
```


```python
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )):  
    plt.figure()  
    plt.title(title)  
    if ylim is not None:  
        plt.ylim(*ylim)  
    plt.xlabel('Training example')  
    plt.ylabel('score')  
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  
    train_scores_mean = np.mean(train_scores, axis=1)  
    train_scores_std = np.std(train_scores, axis=1)  
    test_scores_mean = np.mean(test_scores, axis=1)  
    test_scores_std = np.std(test_scores, axis=1)  
    plt.grid()#åŒºåŸŸ  
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  
                     train_scores_mean + train_scores_std, alpha=0.1,  
                     color="r")  
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  
                     test_scores_mean + test_scores_std, alpha=0.1,  
                     color="g")  
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',  
             label="Training score")  
    plt.plot(train_sizes, test_scores_mean,'o-',color="g",  
             label="Cross-validation score")  
    plt.legend(loc="best")  
    return plt  
```


```python
plot_learning_curve(LinearRegression(), 'Liner_model', train_X[:1000], train_y_ln[:1000], ylim=(0.0, 0.5), cv=5, n_jobs=1)  
```




    <module 'matplotlib.pyplot' from 'C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py'>



![54-1](https://img-blog.csdnimg.cn/20200321231918241.png)


#### 5.4.3 å¤šç§æ¨¡å‹å¯¹æ¯”


```python
train = sample_feature[continuous_feature_names + ['price']].dropna()

train_X = train[continuous_feature_names]
train_y = train['price']
train_y_ln = np.log(train_y + 1)
```

#### 5.4.3 - 1 çº¿æ€§æ¨¡å‹ & åµŒå…¥å¼ç‰¹å¾é€‰æ‹©

æœ¬ç« èŠ‚é»˜è®¤ï¼Œå­¦ä¹ è€…å·²ç»äº†è§£å…³äºè¿‡æ‹Ÿåˆã€æ¨¡å‹å¤æ‚åº¦ã€æ­£åˆ™åŒ–ç­‰æ¦‚å¿µã€‚å¦åˆ™è¯·å¯»æ‰¾ç›¸å…³èµ„æ–™æˆ–å‚è€ƒå¦‚ä¸‹è¿æ¥ï¼š

  - ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€æè¿°ã€Œè¿‡æ‹Ÿåˆ overfittingã€ï¼Ÿ https://www.zhihu.com/question/32246256/answer/55320482
  - æ¨¡å‹å¤æ‚åº¦ä¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ› http://yangyingming.com/article/434/
  - æ­£åˆ™åŒ–çš„ç›´è§‚ç†è§£ https://blog.csdn.net/jinping_shi/article/details/52433975

åœ¨è¿‡æ»¤å¼å’ŒåŒ…è£¹å¼ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸­ï¼Œç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸å­¦ä¹ å™¨è®­ç»ƒè¿‡ç¨‹æœ‰æ˜æ˜¾çš„åˆ†åˆ«ã€‚è€ŒåµŒå…¥å¼ç‰¹å¾é€‰æ‹©åœ¨å­¦ä¹ å™¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨åœ°è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚åµŒå…¥å¼é€‰æ‹©æœ€å¸¸ç”¨çš„æ˜¯L1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ–ã€‚åœ¨å¯¹çº¿æ€§å›å½’æ¨¡å‹åŠ å…¥ä¸¤ç§æ­£åˆ™åŒ–æ–¹æ³•åï¼Œä»–ä»¬åˆ†åˆ«å˜æˆäº†å²­å›å½’ä¸Lassoå›å½’ã€‚


```python
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
```


```python
models = [LinearRegression(),
          Ridge(),
          Lasso()]
```


```python
result = dict()
for model in models:
    model_name = str(model).split('(')[0]
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))
    result[model_name] = scores
    print(model_name + ' is finished')
```

    LinearRegression is finished
    Ridge is finished
    Lasso is finished
    

å¯¹ä¸‰ç§æ–¹æ³•çš„æ•ˆæœå¯¹æ¯”


```python
result = pd.DataFrame(result)
result.index = ['cv' + str(x) for x in range(1, 6)]
result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>Ridge</th>
      <th>Lasso</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cv1</th>
      <td>0.191642</td>
      <td>0.195665</td>
      <td>0.382708</td>
    </tr>
    <tr>
      <th>cv2</th>
      <td>0.194986</td>
      <td>0.198841</td>
      <td>0.383916</td>
    </tr>
    <tr>
      <th>cv3</th>
      <td>0.192737</td>
      <td>0.196629</td>
      <td>0.380754</td>
    </tr>
    <tr>
      <th>cv4</th>
      <td>0.195329</td>
      <td>0.199255</td>
      <td>0.385683</td>
    </tr>
    <tr>
      <th>cv5</th>
      <td>0.194450</td>
      <td>0.198173</td>
      <td>0.383555</td>
    </tr>
  </tbody>
</table>
</div>




```python
model = LinearRegression().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:23.515984499017883
    




    <matplotlib.axes._subplots.AxesSubplot at 0x1feb933ca58>




![output_65_2](https://img-blog.csdnimg.cn/20200321231945959.png)


L2æ­£åˆ™åŒ–åœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­é€šå¸¸éƒ½å€¾å‘äºè®©æƒå€¼å°½å¯èƒ½å°ï¼Œæœ€åæ„é€ ä¸€ä¸ªæ‰€æœ‰å‚æ•°éƒ½æ¯”è¾ƒå°çš„æ¨¡å‹ã€‚å› ä¸ºä¸€èˆ¬è®¤ä¸ºå‚æ•°å€¼å°çš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œèƒ½é€‚åº”ä¸åŒçš„æ•°æ®é›†ï¼Œä¹Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚å¯ä»¥è®¾æƒ³ä¸€ä¸‹å¯¹äºä¸€ä¸ªçº¿æ€§å›å½’æ–¹ç¨‹ï¼Œè‹¥å‚æ•°å¾ˆå¤§ï¼Œé‚£ä¹ˆåªè¦æ•°æ®åç§»ä¸€ç‚¹ç‚¹ï¼Œå°±ä¼šå¯¹ç»“æœé€ æˆå¾ˆå¤§çš„å½±å“ï¼›ä½†å¦‚æœå‚æ•°è¶³å¤Ÿå°ï¼Œæ•°æ®åç§»å¾—å¤šä¸€ç‚¹ä¹Ÿä¸ä¼šå¯¹ç»“æœé€ æˆä»€ä¹ˆå½±å“ï¼Œä¸“ä¸šä¸€ç‚¹çš„è¯´æ³•æ˜¯ã€æŠ—æ‰°åŠ¨èƒ½åŠ›å¼ºã€


```python
model = Ridge().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:5.901527844424091
    




    <matplotlib.axes._subplots.AxesSubplot at 0x1fea9056860>




![output_67_2](https://img-blog.csdnimg.cn/20200321232121957.png)


L1æ­£åˆ™åŒ–æœ‰åŠ©äºç”Ÿæˆä¸€ä¸ªç¨€ç–æƒå€¼çŸ©é˜µï¼Œè¿›è€Œå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ã€‚å¦‚ä¸‹å›¾ï¼Œæˆ‘ä»¬å‘ç°powerä¸userd_timeç‰¹å¾éå¸¸é‡è¦ã€‚


```python
model = Lasso().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:8.674427764003347
    




    <matplotlib.axes._subplots.AxesSubplot at 0x1fea90b69b0>



![output_69_2](https://img-blog.csdnimg.cn/202003212321463.png)


é™¤æ­¤ä¹‹å¤–ï¼Œå†³ç­–æ ‘é€šè¿‡ä¿¡æ¯ç†µæˆ–GINIæŒ‡æ•°é€‰æ‹©åˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©çš„åˆ†è£‚ç‰¹å¾ä¹Ÿæ›´åŠ é‡è¦ï¼Œè¿™åŒæ ·æ˜¯ä¸€ç§ç‰¹å¾é€‰æ‹©çš„æ–¹æ³•ã€‚XGBoostä¸LightGBMæ¨¡å‹ä¸­çš„model_importanceæŒ‡æ ‡æ­£æ˜¯åŸºäºæ­¤è®¡ç®—çš„

#### 5.4.3 - 2 éçº¿æ€§æ¨¡å‹

é™¤äº†çº¿æ€§æ¨¡å‹ä»¥å¤–ï¼Œè¿˜æœ‰è®¸å¤šæˆ‘ä»¬å¸¸ç”¨çš„éçº¿æ€§æ¨¡å‹å¦‚ä¸‹ï¼Œåœ¨æ­¤ç¯‡å¹…æœ‰é™ä¸å†ä¸€ä¸€è®²è§£åŸç†ã€‚æˆ‘ä»¬é€‰æ‹©äº†éƒ¨åˆ†å¸¸ç”¨æ¨¡å‹ä¸çº¿æ€§æ¨¡å‹è¿›è¡Œæ•ˆæœæ¯”å¯¹ã€‚


```python
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from xgboost.sklearn import XGBRegressor
from lightgbm.sklearn import LGBMRegressor
```


```python
models = [LinearRegression(),
          DecisionTreeRegressor(),
          RandomForestRegressor(),
          GradientBoostingRegressor(),
          MLPRegressor(solver='lbfgs', max_iter=100), 
          XGBRegressor(n_estimators = 100, objective='reg:squarederror'), 
          LGBMRegressor(n_estimators = 100)]
```


```python
result = dict()
for model in models:
    model_name = str(model).split('(')[0]
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))
    result[model_name] = scores
    print(model_name + ' is finished')
```

    LinearRegression is finished
    DecisionTreeRegressor is finished
    RandomForestRegressor is finished
    GradientBoostingRegressor is finished
    MLPRegressor is finished
    XGBRegressor is finished
    LGBMRegressor is finished
    


```python
result = pd.DataFrame(result)
result.index = ['cv' + str(x) for x in range(1, 6)]
result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>DecisionTreeRegressor</th>
      <th>RandomForestRegressor</th>
      <th>GradientBoostingRegressor</th>
      <th>MLPRegressor</th>
      <th>XGBRegressor</th>
      <th>LGBMRegressor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cv1</th>
      <td>0.191642</td>
      <td>0.184566</td>
      <td>0.136266</td>
      <td>0.168626</td>
      <td>124.299426</td>
      <td>0.168698</td>
      <td>0.141159</td>
    </tr>
    <tr>
      <th>cv2</th>
      <td>0.194986</td>
      <td>0.187029</td>
      <td>0.139693</td>
      <td>0.171905</td>
      <td>257.886236</td>
      <td>0.172258</td>
      <td>0.143363</td>
    </tr>
    <tr>
      <th>cv3</th>
      <td>0.192737</td>
      <td>0.184839</td>
      <td>0.136871</td>
      <td>0.169553</td>
      <td>236.829589</td>
      <td>0.168604</td>
      <td>0.142137</td>
    </tr>
    <tr>
      <th>cv4</th>
      <td>0.195329</td>
      <td>0.182605</td>
      <td>0.138689</td>
      <td>0.172299</td>
      <td>130.197264</td>
      <td>0.172474</td>
      <td>0.143461</td>
    </tr>
    <tr>
      <th>cv5</th>
      <td>0.194450</td>
      <td>0.186626</td>
      <td>0.137420</td>
      <td>0.171206</td>
      <td>268.090236</td>
      <td>0.170898</td>
      <td>0.141921</td>
    </tr>
  </tbody>
</table>
</div>



å¯ä»¥çœ‹åˆ°éšæœºæ£®æ—æ¨¡å‹åœ¨æ¯ä¸€ä¸ªfoldä¸­å‡å–å¾—äº†æ›´å¥½çš„æ•ˆæœ

#### 5.4.4  æ¨¡å‹è°ƒå‚

åœ¨æ­¤æˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§å¸¸ç”¨çš„è°ƒå‚æ–¹æ³•å¦‚ä¸‹ï¼š

  - è´ªå¿ƒç®—æ³• https://www.jianshu.com/p/ab89df9759c8
  - ç½‘æ ¼è°ƒå‚ https://blog.csdn.net/weixin_43172660/article/details/83032029
  - è´å¶æ–¯è°ƒå‚ https://blog.csdn.net/linxid/article/details/81189154


```python
## LGBçš„å‚æ•°é›†åˆï¼š

objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']

num_leaves = [3,5,10,15,20,40, 55]
max_depth = [3,5,10,15,20,40, 55]
bagging_fraction = []
feature_fraction = []
drop_rate = []
```

#### 5.4.4 - 1 è´ªå¿ƒè°ƒå‚


```python
best_obj = dict()
for obj in objective:
    model = LGBMRegressor(objective=obj)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_obj[obj] = score
    
best_leaves = dict()
for leaves in num_leaves:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_leaves[leaves] = score
    
best_depth = dict()
for depth in max_depth:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],
                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],
                          max_depth=depth)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_depth[depth] = score
```


```python
sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1fea93f6080>




![83-1](https://img-blog.csdnimg.cn/20200321232159934.png)


#### 5.4.4 - 2 Grid Search è°ƒå‚


```python
from sklearn.model_selection import GridSearchCV
```


```python
parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}
model = LGBMRegressor()
clf = GridSearchCV(model, parameters, cv=5)
clf = clf.fit(train_X, train_y)
```


```python
clf.best_params_
```




    {'max_depth': 15, 'num_leaves': 55, 'objective': 'regression'}




```python
model = LGBMRegressor(objective='regression',
                          num_leaves=55,
                          max_depth=15)
```


```python
np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
```




    0.13626164479243302



#### 5.4.4 - 3 è´å¶æ–¯è°ƒå‚


```python
from bayes_opt import BayesianOptimization
```


```python
def rf_cv(num_leaves, max_depth, subsample, min_child_samples):
    val = cross_val_score(
        LGBMRegressor(objective = 'regression_l1',
            num_leaves=int(num_leaves),
            max_depth=int(max_depth),
            subsample = subsample,
            min_child_samples = int(min_child_samples)
        ),
        X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)
    ).mean()
    return 1 - val
```


```python
rf_bo = BayesianOptimization(
    rf_cv,
    {
    'num_leaves': (2, 100),
    'max_depth': (2, 100),
    'subsample': (0.1, 1),
    'min_child_samples' : (2, 100)
    }
)
```


```python
rf_bo.maximize()
```

    |   iter    |  target   | max_depth | min_ch... | num_le... | subsample |
    -------------------------------------------------------------------------
    | [0m 1       [0m | [0m 0.8649  [0m | [0m 89.57   [0m | [0m 47.3    [0m | [0m 55.13   [0m | [0m 0.1792  [0m |
    | [0m 2       [0m | [0m 0.8477  [0m | [0m 99.86   [0m | [0m 60.91   [0m | [0m 15.35   [0m | [0m 0.4716  [0m |
    | [95m 3       [0m | [95m 0.8698  [0m | [95m 81.74   [0m | [95m 83.32   [0m | [95m 92.59   [0m | [95m 0.9559  [0m |
    | [0m 4       [0m | [0m 0.8627  [0m | [0m 90.2    [0m | [0m 8.754   [0m | [0m 43.34   [0m | [0m 0.7772  [0m |
    | [0m 5       [0m | [0m 0.8115  [0m | [0m 10.07   [0m | [0m 86.15   [0m | [0m 4.109   [0m | [0m 0.3416  [0m |
    | [95m 6       [0m | [95m 0.8701  [0m | [95m 99.15   [0m | [95m 9.158   [0m | [95m 99.47   [0m | [95m 0.494   [0m |
    | [0m 7       [0m | [0m 0.806   [0m | [0m 2.166   [0m | [0m 2.416   [0m | [0m 97.7    [0m | [0m 0.224   [0m |
    | [0m 8       [0m | [0m 0.8701  [0m | [0m 98.57   [0m | [0m 97.67   [0m | [0m 99.87   [0m | [0m 0.3703  [0m |
    | [95m 9       [0m | [95m 0.8703  [0m | [95m 99.87   [0m | [95m 43.03   [0m | [95m 99.72   [0m | [95m 0.9749  [0m |
    | [0m 10      [0m | [0m 0.869   [0m | [0m 10.31   [0m | [0m 99.63   [0m | [0m 99.34   [0m | [0m 0.2517  [0m |
    | [95m 11      [0m | [95m 0.8703  [0m | [95m 52.27   [0m | [95m 99.56   [0m | [95m 98.97   [0m | [95m 0.9641  [0m |
    | [0m 12      [0m | [0m 0.8669  [0m | [0m 99.89   [0m | [0m 8.846   [0m | [0m 66.49   [0m | [0m 0.1437  [0m |
    | [0m 13      [0m | [0m 0.8702  [0m | [0m 68.13   [0m | [0m 75.28   [0m | [0m 98.71   [0m | [0m 0.153   [0m |
    | [0m 14      [0m | [0m 0.8695  [0m | [0m 84.13   [0m | [0m 86.48   [0m | [0m 91.9    [0m | [0m 0.7949  [0m |
    | [0m 15      [0m | [0m 0.8702  [0m | [0m 98.09   [0m | [0m 59.2    [0m | [0m 99.65   [0m | [0m 0.3275  [0m |
    | [0m 16      [0m | [0m 0.87    [0m | [0m 68.97   [0m | [0m 98.62   [0m | [0m 98.93   [0m | [0m 0.2221  [0m |
    | [0m 17      [0m | [0m 0.8702  [0m | [0m 99.85   [0m | [0m 63.74   [0m | [0m 99.63   [0m | [0m 0.4137  [0m |
    | [0m 18      [0m | [0m 0.8703  [0m | [0m 45.87   [0m | [0m 99.05   [0m | [0m 99.89   [0m | [0m 0.3238  [0m |
    | [0m 19      [0m | [0m 0.8702  [0m | [0m 79.65   [0m | [0m 46.91   [0m | [0m 98.61   [0m | [0m 0.8999  [0m |
    | [0m 20      [0m | [0m 0.8702  [0m | [0m 99.25   [0m | [0m 36.73   [0m | [0m 99.05   [0m | [0m 0.1262  [0m |
    | [0m 21      [0m | [0m 0.8702  [0m | [0m 85.51   [0m | [0m 85.34   [0m | [0m 99.77   [0m | [0m 0.8917  [0m |
    | [0m 22      [0m | [0m 0.8696  [0m | [0m 99.99   [0m | [0m 38.51   [0m | [0m 89.13   [0m | [0m 0.9884  [0m |
    | [0m 23      [0m | [0m 0.8701  [0m | [0m 63.29   [0m | [0m 97.93   [0m | [0m 99.94   [0m | [0m 0.9585  [0m |
    | [0m 24      [0m | [0m 0.8702  [0m | [0m 93.04   [0m | [0m 71.42   [0m | [0m 99.94   [0m | [0m 0.9646  [0m |
    | [0m 25      [0m | [0m 0.8701  [0m | [0m 99.73   [0m | [0m 16.21   [0m | [0m 99.38   [0m | [0m 0.9778  [0m |
    | [0m 26      [0m | [0m 0.87    [0m | [0m 86.28   [0m | [0m 58.1    [0m | [0m 99.47   [0m | [0m 0.107   [0m |
    | [0m 27      [0m | [0m 0.8703  [0m | [0m 47.28   [0m | [0m 99.83   [0m | [0m 99.65   [0m | [0m 0.4674  [0m |
    | [0m 28      [0m | [0m 0.8703  [0m | [0m 68.29   [0m | [0m 99.51   [0m | [0m 99.4    [0m | [0m 0.2757  [0m |
    | [0m 29      [0m | [0m 0.8701  [0m | [0m 76.49   [0m | [0m 73.41   [0m | [0m 99.86   [0m | [0m 0.9394  [0m |
    | [0m 30      [0m | [0m 0.8695  [0m | [0m 37.27   [0m | [0m 99.87   [0m | [0m 89.87   [0m | [0m 0.7588  [0m |
    =========================================================================
    


```python
1 - rf_bo.max['target']
```




    0.1296693644053145



## æ€»ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å®Œæˆäº†å»ºæ¨¡ä¸è°ƒå‚çš„å·¥ä½œï¼Œå¹¶å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº†éªŒè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€äº›åŸºæœ¬æ–¹æ³•æ¥æé«˜é¢„æµ‹çš„ç²¾åº¦ï¼Œæå‡å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚


```python
plt.figure(figsize=(13,5))
sns.lineplot(x=['0_origin','1_log_transfer','2_L1_&_L2','3_change_model','4_parameter_turning'], y=[1.36 ,0.19, 0.19, 0.14, 0.13])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1feac73ceb8>




![98-1](https://img-blog.csdnimg.cn/20200321232216795.png)


**Task5 å»ºæ¨¡è°ƒå‚ END.**

--- By: å°é›¨å§‘å¨˜  

    æ•°æ®æŒ–æ˜çˆ±å¥½è€…ï¼Œå¤šæ¬¡è·æ¯”èµ›TOPåæ¬¡ã€‚
    ä½œè€…çš„æœºå™¨å­¦ä¹ ç¬”è®°ï¼šhttps://zhuanlan.zhihu.com/mlbasic

**å…³äºDatawhaleï¼š**

> Datawhaleæ˜¯ä¸€ä¸ªä¸“æ³¨äºæ•°æ®ç§‘å­¦ä¸AIé¢†åŸŸçš„å¼€æºç»„ç»‡ï¼Œæ±‡é›†äº†ä¼—å¤šé¢†åŸŸé™¢æ ¡å’ŒçŸ¥åä¼ä¸šçš„ä¼˜ç§€å­¦ä¹ è€…ï¼Œèšåˆäº†ä¸€ç¾¤æœ‰å¼€æºç²¾ç¥å’Œæ¢ç´¢ç²¾ç¥çš„å›¢é˜Ÿæˆå‘˜ã€‚Datawhale ä»¥â€œfor the learnerï¼Œå’Œå­¦ä¹ è€…ä¸€èµ·æˆé•¿â€ä¸ºæ„¿æ™¯ï¼Œé¼“åŠ±çœŸå®åœ°å±•ç°è‡ªæˆ‘ã€å¼€æ”¾åŒ…å®¹ã€äº’ä¿¡äº’åŠ©ã€æ•¢äºè¯•é”™å’Œå‹‡äºæ‹…å½“ã€‚åŒæ—¶ Datawhale ç”¨å¼€æºçš„ç†å¿µå»æ¢ç´¢å¼€æºå†…å®¹ã€å¼€æºå­¦ä¹ å’Œå¼€æºæ–¹æ¡ˆï¼Œèµ‹èƒ½äººæ‰åŸ¹å…»ï¼ŒåŠ©åŠ›äººæ‰æˆé•¿ï¼Œå»ºç«‹èµ·äººä¸äººï¼Œäººä¸çŸ¥è¯†ï¼Œäººä¸ä¼ä¸šå’Œäººä¸æœªæ¥çš„è”ç»“ã€‚

æœ¬æ¬¡æ•°æ®æŒ–æ˜è·¯å¾„å­¦ä¹ ï¼Œä¸“é¢˜çŸ¥è¯†å°†åœ¨å¤©æ± åˆ†äº«ï¼Œè¯¦æƒ…å¯å…³æ³¨Datawhaleï¼š

![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584426326920_9FOUExG2be.jpg)
